from evalscope import TaskConfig, run_task
from evalscope.constants import EvaluationMode

# 自定义多维度评测模板
MULTI_DIMENSION_PROMPT = """
作为专业评估员，请根据以下标准对模型回答进行多维度评分。请遵循思维链推理原则，逐步分析后给出评分。

# 评分维度（1-5分）
1. **相关性(Relevance)**：回答是否直接解决核心问题
   - 5: 完全相关且聚焦主题
   - 3: 基本相关但有少量偏离
   - 1: 完全偏离主题

2. **准确性(Accuracy)**：事实、逻辑与参考答案的一致性
   - 5: 全部信息准确无误
   - 3: 主要观点正确但细节有误
   - 1: 关键事实错误

3. **完整性(Completeness)**：覆盖参考答案要点的程度
   - 5: 覆盖所有核心要点
   - 3: 覆盖主要要点但遗漏次要
   - 1: 遗漏核心要点

4. **逻辑性(Logicality)**：论证结构的严谨程度
   - 5: 推理严密无矛盾
   - 3: 基本连贯但有小漏洞
   - 1: 逻辑混乱

5. **表达质量(Expression)**：语言流畅性与专业性
   - 5: 专业表达无语法错误
   - 3: 基本通顺有个别问题
   - 1: 难以理解的表达

# 评估对象
问题：{question}
参考答案：{reference_answer}
待评估回答：{model_response}

# 评估要求
1. 按[维度→分析→评分]格式逐步推理
2. 最终输出严格遵循JSON格式：
{{
  "reasoning": "推理过程文本",
  "scores": {{
    "relevance": 分数,
    "accuracy": 分数,
    "completeness": 分数,
    "logicality": 分数,
    "expression": 分数
  }},
  "overall": 平均分
}}
"""

# 配置评测任务
task_cfg = TaskConfig(
    # 被评测模型配置
    model="your_model_path_or_api",  # 替换为实际模型路径或API名称
    model_args={
        "temperature": 0.7,
        "max_tokens": 1024
    },
    
    # 评测数据集配置
    datasets=[
        {
            "name": "your_custom_dataset",
            "path": "path/to/your/dataset.json",  # 自定义数据路径
            "format": {
                "question": "question_field",
                "reference_answer": "reference_field",
                "model_response": "response_field"
            }
        }
    ],
    
    # LLM-Judge 配置
    evaluation_mode=EvaluationMode.LLM_AS_JUDGE,
    judge_model="gpt-4-turbo",  # 推荐使用高性能裁判模型[1,7](@ref)
    judge_config={
        "temperature": 0.3,  # 低温度保证稳定性
        "response_format": {"type": "json_object"}
    },
    evaluation_prompt=MULTI_DIMENSION_PROMPT,
    
    # 多维度评分配置
    metrics=[
        {"name": "relevance", "range": [1, 5]},
        {"name": "accuracy", "range": [1, 5]},
        {"name": "completeness", "range": [1, 5]},
        {"name": "logicality", "range": [1, 5]},
        {"name": "expression", "range": [1, 5]},
        {"name": "overall", "calculation": "mean"}
    ],
    
    # 高级配置
    evaluation_rubrics={
        "position_bias_mitigation": "参考答案置于模型回答后",
        "length_bias_mitigation": "明确要求忽略回答长度专注质量"
    },
    batch_size=5,  # 并行评测数量
    max_samples=100,  # 最大评测样本数
    analysis_report=True  # 生成分析报告
)

# 运行评测任务
results = run_task(task_cfg)

# 结果解析示例
print(f"评测完成！共评估 {len(results)} 个样本")
print(f"平均综合得分: {results['metrics']['overall']['mean']:.2f}/5")

# 维度分析
print("\n各维度平均分:")
for dim in ['relevance', 'accuracy', 'completeness', 'logicality', 'expression']:
    print(f"- {dim.capitalize()}: {results['metrics'][dim]['mean']:.2f}/5")

# 生成可视化报告
results.visualize(
    output_format="html",
    output_path="model_evaluation_report.html"
)
print("可视化报告已生成: model_evaluation_report.html")
